import torch
import torch.nn.functional as F
from my_tokenizer import CharTokenizer, BPETokenizer
from model import tinyLLM

def load_model_and_tokenizer(checkpoint_path='C:\\Users\\Admin\\Desktop\\AI\\python\\LLM\\tinyllm_checkpoint.pth', 
                             input_path='C:\\Users\\Admin\\Desktop\\AI\\python\\LLM\\input.txt'):
    """Load the trained model and tokenizer"""
    print("ðŸ”„ Loading model and tokenizer...")
    
    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # Load tokenizer from training data
    with open(input_path, 'r', encoding='utf-8') as f:
        text = f.read()
    tokenizer = BPETokenizer() #CharTokenizer(text)
    
    # Create and load model
    model = tinyLLM(
        vocab_size=checkpoint['vocab_size'],
        embed_dim=checkpoint['embed_dim'],
        block_size=checkpoint['block_size'],
        num_blocks=checkpoint['num_blocks']
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # Move to device
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    model.eval()
    
    print(f"âœ… Model loaded! Vocabulary: {tokenizer.vocab_size} chars")
    return model, tokenizer, device

def generate_text(model, tokenizer, prompt, max_new_tokens=80, temperature=0.7, device='cpu'):
    """Generate text using sampling"""
    model.eval()
    idx = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)
    
    with torch.no_grad():
        for _ in range(max_new_tokens):
            idx_cond = idx if idx.size(1) <= model.block_size else idx[:, -model.block_size:]
            logits, _ = model(idx_cond)
            logits = logits[:, -1, :] / temperature
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, next_token), dim=1)
    
    return tokenizer.decode(idx[0].tolist())

def generate_greedy(model, tokenizer, prompt, max_new_tokens=80, device='cpu'):
    """Generate text using greedy decoding"""
    model.eval()
    idx = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)
    
    with torch.no_grad():
        for _ in range(max_new_tokens):
            idx_cond = idx if idx.size(1) <= model.block_size else idx[:, -model.block_size:]
            logits, _ = model(idx_cond)
            next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(1)
            idx = torch.cat((idx, next_token), dim=1)
    
    return tokenizer.decode(idx[0].tolist())

def test_model():
    """Complete test function"""
    print("ðŸ§ª TESTING YOUR POETRY MODEL")
    print("=" * 60)
    
    # Load model
    model, tokenizer, device = load_model_and_tokenizer()
    
    # Test prompts
    prompts = [
        "The moon rises over the forest where daffodils bloom",
        "To be or not to be: that is the question of mortal being",
        "Once upon a time in a green valley",
        "I wandered lonely as a cloud",
        "The wind whispers",
        "Love is what light is",
        "Darkness falls when the helo goes down",
        "In the garden of my devotion",
        "The king longed with a sigh",
        "Where art thou",
    ]
    
    print(f"\nðŸ“ Testing with {len(prompts)} different prompts:\n")
    
    for i, prompt in enumerate(prompts, 1):
        print(f"Prompt {i}: {repr(prompt)}")
        
        # Greedy generation
        greedy_output = generate_greedy(model, tokenizer, prompt, max_new_tokens=60, device=device)
        print(f"Greedy:  {repr(greedy_output[len(prompt):])}")
        
        # Sampling generation
        sample_output = generate_text(model, tokenizer, prompt, max_new_tokens=60, temperature=0.7, device=device)
        print(f"Sample:  {repr(sample_output[len(prompt):])}")
        
        print("-" * 60)
    
    # Interactive mode
    print("\nðŸ’¬ INTERACTIVE MODE - Type your own prompts (or 'quit' to exit)")
    print("Examples: 'The sun', 'In spring', 'A rose', etc.\n")
    
    while True:
        user_prompt = input("Your prompt: ").strip()
        if user_prompt.lower() in ['quit', 'exit', 'q']:
            break
        if not user_prompt:
            continue
            
        greedy_result = generate_greedy(model, tokenizer, user_prompt, max_new_tokens=80, device=device)
        sample_result = generate_text(model, tokenizer, user_prompt, max_new_tokens=80, temperature=0.7, device=device)
        
        print(f"Greedy:  {greedy_result[len(user_prompt):]}")
        print(f"Sample:  {sample_result[len(user_prompt):]}")
        print()

if __name__ == "__main__":
    test_model()